# ğŸ™ï¸ Whisper.Mojo

A high-performance implementation of OpenAI's **Whisper** model (Tiny version) written entirely in **Mojo** ğŸ”¥.

## ğŸš€ Overview

This project brings the power of OpenAI's Whisper to the Mojo programming language. By implementing the architecture from the ground up, we leverage Mojo's unique ability to combine Python-like syntax with C-level performance through hardware acceleration, SIMD, and low-level memory control.

> [!NOTE] 
> This implementation currently supports **Whisper-Tiny** with greedy decoding for English transcription.

## âœ¨ Features

- **ğŸ¯ Pure Mojo Implementation**: Every layer (Encoder, Decoder, Multi-Head Attention) is written in Mojo.
- **ğŸš€ Ultra-Fast Inference**: Uses **KV-Caching** for incremental decoding, reducing complexity from $O(L^2)$ to $O(L)$.
- **ğŸ§µ Multi-core Parallelization**: Parallelized attention heads and tensor operations using Mojo's `parallelize` algorithm.
- **âš¡ SIMD Acceleration**: Core math operations (Matmul, LayerNorm, GeLU) are vectorized using Mojo's SIMD primitives.
- **ğŸ§ Real-world Audio**: Integrated pipeline to process real audio files (MP3/WAV) into Mel spectrograms.
- **ğŸ” Bit-Perfect Tokenization**: Fully compatible with OpenAI's tokenizer, producing identical results to the PyTorch reference implementation.

## ğŸ“‚ Project Structure

| File | Description |
| :--- | :--- |
| `main.mojo` | ğŸ® The entry point. Orchestrates weight loading, audio processing, and transcription. |
| `whisper.mojo` | ğŸ§  The "Brain". Contains the `Whisper` model and incremental decoding logic. |
| `layers.mojo` | ğŸ§± Core building blocks with **KVCache** support and parallelized attention. |
| `whisper_tensor.mojo` | ğŸ§¬ Mathematical foundation. Implements parallelized & SIMD-optimized tensor ops. |
| `tokenizer.mojo` | ğŸ”¤ Decodes numeric tokens into human-readable text. |
| `loader.mojo` | ğŸ“¥ Efficient binary weight loader. |
| `export_weights.py` | ğŸ Python bridge for weight export and audio preprocessing. |

## ğŸ› ï¸ Getting Started

### ğŸ“‹ Prerequisites

- **Mojo SDK** (v24.5+)
- **Python Environment** with `torch`, `transformers`, `soundfile`, `scipy`, `requests`

### ğŸ—ï¸ Installation & Execution

1. **Clone & Setup**
   ```bash
   git clone https://github.com/antonvice/whisper.Mojo.git
   cd whisper.Mojo
   ```

2. **Prepare Weights & Audio**
   ```bash
   uv run export_weights.py
   ```

3. **Build & Run (Recommended for Speed)**
   For the best performance, compile to a native binary:
   ```bash
   mojo build main.mojo
   ./main
   ```

## ğŸ“Š Optimization Details

This implementation is designed to showcase Mojo's performance advantages:

1. **KV-Cache**: Instead of re-computing the entire sequence for every new token, we cache the keys and values of previous tokens.
2. **Parallel Heads**: All attention heads in a layer are processed simultaneously on multiple CPU cores.
3. **SIMD Vectorization**: Inner loops are manually tuned to use 256-bit or 512-bit registers (depending on hardware).

## ğŸ“ Example Output

```text
Initializing Whisper Tiny in Mojo...
Loading weights from whisper_tiny_weights.bin...
Transcription:
--------------------
 This is my voice on the left. This is my voice on the left hand side...
--------------------
```

## ğŸ“ˆ Changelog

### [2025-12-29] - ğŸš€ Performance Breakthrough: Sub-Python Latency
- **ğŸ Beat Python Baseline**: Achieved a total transcription time of **0.74s**, outperforming the Python/PyTorch reference implementation (0.78s).
- **ğŸï¸ Register-Heavy Decoder**: Implemented a peak-performance decoder attention path using register-cached heads. Optimized decoding by switching to serial head loops, eliminating thread pool overhead for small tasks.
- **ğŸ”„ Contiguous Encoder Pipeline**: Redesigned the encoder to use transposed-output convolutions. Implemented **Blocked Parallelization** (grain size 16) in `conv1d` to eliminate cache-line contention (false sharing).
- **ğŸ§µ Parallel Logit Projection**: Optimized the final $1 \times 51k$ output layer to parallelize across all cores, maximizing throughput for small batch sizes.
- **ğŸ§± MAX Engine Integration**: Leveraged Modular's MAX Engine specialized matmul kernels for all encoder Transformer blocks.
- **ğŸ›¡ï¸ Warning Cleanup**: Resolved compiler warnings in `Tensor.__moveinit__` while ensuring proper move semantics.

### [2025-12-26] - Performance Optimization Sprint (Part 2)
- **ğŸš€ Advanced `conv1d` Vectorization**: Implemented a "Transpose-DotProduct" strategy for 1D convolutions, enabling full SIMD utilization. Optimized core Whisper filters (K=3) with manual unrolling and hoisting of accumulation logic.
- **âš¡ Matrix-Matrix Matmul Tiling**: Enhanced the matrix multiplication kernel with 8x tiling and unrolling for the $N$ dimension. This significantly reduced memory pressure and improved throughput for large encoder blocks ($M=1500$).
- **ğŸ§¬ Optimized Prefill (Attention)**: Optimized the prefill/encoder path by switching from manual scalar loops to high-performance `matmul`-based head processing. Added parallelized extraction and scatter of attention heads.
- **ğŸ’¾ Layout-Aware Weight Loading**: Integrated pre-transposition of convolutional weights during model loading to ensure optimal memory layout for inference.
- **ğŸ›¡ï¸ Robust SIMD Kernels**: Implemented generalized tail-handling in `matmul` and `conv1d`, ensuring stability across arbitrary sequence lengths and filter sizes.
- **ğŸ“ˆ Benchmark Results**: Successfully reduced total transcription time to **~1.59s** (from ~3.3s), achieving a **2x overall speedup**. Encoder runtime reduced by over 35%.

### [2025-12-25] - Performance Optimization Sprint (Part 1)
*   **ğŸš€ Optimized Matmul**: Implemented dynamic parallelization that adapts to matrix shapes. Added 1D tiling for better cache reuse and switched to hardware-native SIMD widths using `simdwidthof`.
*   **âš¡ Vectorized Attention**: Fully vectorized the inner loops of `MultiHeadAttention`, accelerating both the dot-product score calculation and the weighted value sum.
*   **ğŸ§¬ Optimized Tensor Primitives**: Vectorized `LayerNorm`, `Softmax`, and `GeLU` operations. Added safe tail-handling for non-multiple sequence lengths.
*   **ğŸ’¾ Fast Memory Operations**: Replaced slow scalar loops in KV-cache management with high-performance `memcpy` transfers.
*   **ğŸ§µ Threading Improvements**: Optimized thread distribution in decoder layers to ensure all CPU cores are utilized during incremental decoding (single-token generation).

## ğŸ“œ License

This project is licensed under the MIT License - see the LICENSE file for details.
